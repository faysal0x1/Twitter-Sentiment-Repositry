{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial_id",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4360eab3447d55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T05:24:27.314833300Z",
     "start_time": "2024-02-16T05:24:24.379097Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Read Data\n",
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv', header=None, encoding='latin')\n",
    "\n",
    "df.columns = ['label', 'id', 'date', 'query', 'user', 'tweet']\n",
    "\n",
    "# Data reduction\n",
    "df = df.drop(['id', 'date', 'query', 'user'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396c45d76918f615",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T05:24:27.603643200Z",
     "start_time": "2024-02-16T05:24:27.316835800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "labels_dict = {0:'Negative', 2:'Neutral', 4:'Positive'}\n",
    "\n",
    "def convert_labels(label):\n",
    "    return labels_dict[label]\n",
    "\n",
    "df.label = df.label.apply(lambda x: convert_labels(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bc37f2-4787-4d3c-a49b-7c0559cfb07f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T19:06:36.840469800Z",
     "start_time": "2024-05-06T19:06:35.132981500Z"
    }
   },
   "outputs": [],
   "source": [
    "instances = df.label.value_counts()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(instances.index, instances.values)\n",
    "plt.title(\"Data Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9648939b-34ec-4587-8511-76d19bfc139b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-16T05:24:27.853386500Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "punctuations_and_dummies = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "def preprocess(df, will_be_stemmed=False):\n",
    "    for index, row in df.iterrows():\n",
    "        tweet = row.tweet\n",
    "        tweet = re.sub(punctuations_and_dummies, ' ', str(tweet).lower()).strip()\n",
    "        tokens = []\n",
    "        for token in tweet.split():\n",
    "            if token not in stop_words:\n",
    "                if will_be_stemmed:\n",
    "                    tokens.append(stemmer.stem(token))\n",
    "                else:\n",
    "                    tokens.append(token)\n",
    "        df.tweet = \" \".join(tokens)\n",
    "\n",
    "\n",
    "preprocess(df.tweet)\n",
    "'''\n",
    "\n",
    "\n",
    "def preprocess(tweet, will_be_stemmed=False):\n",
    "        tweet = re.sub(punctuations_and_dummies, ' ', str(tweet).lower()).strip()\n",
    "        tokens = []\n",
    "        for token in tweet.split():\n",
    "            if token not in stop_words:\n",
    "                if will_be_stemmed:\n",
    "                    tokens.append(stemmer.stem(token))\n",
    "                else:\n",
    "                    tokens.append(token)\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "df.tweet = df.tweet.apply(lambda tw: preprocess(tw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b71f7b76-5cf3-4a18-9953-c2e3ff240fdd",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Remove 0 length tweets\n",
    "df = df[df.iloc[:,1].astype(str).str.len()!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aefc5a0-cbb1-434d-b6a9-0e4e871b0a0c",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "tweets_len = [len(x) for x in df['tweet']]\n",
    "pd.Series(tweets_len).hist()\n",
    "plt.show()\n",
    "pd.Series(tweets_len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "847fa6e5-5c85-4e54-97d8-2dcbf2eb0f67",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "all_str = \"\"\n",
    "for i in df.tweet:\n",
    "    all_str += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a682218-83f2-4c49-8257-fc2808b4ead1",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "letter_list = list(all_str)\n",
    "my_counter = Counter(letter_list)\n",
    "\n",
    "letter_df = pd.DataFrame.from_dict(my_counter, orient='index').reset_index()\n",
    "letter_df = letter_df.rename(columns={'index':'letter', 0:'frequency'})\n",
    "letter_df = letter_df.loc[letter_df['letter'].isin(['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z'])]\n",
    "letter_df['all_tweets_relative_freq']=letter_df['frequency']/letter_df['frequency'].sum()\n",
    "letter_df = letter_df.sort_values('letter')\n",
    "\n",
    "english = pd.read_csv('letter_frequency_en_US.csv')\n",
    "english['expected_relative_frequency'] = english['count']/english['count'].sum()\n",
    "english = english.drop(['count'], axis=1)\n",
    "\n",
    "letter_df = pd.merge(letter_df, english, on='letter')\n",
    "letter_df['expected'] = np.round(letter_df['expected_relative_frequency']*letter_df['frequency'].sum(),0)\n",
    "letter_df = letter_df.reset_index().drop(['index'], axis=1)\n",
    "letter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065782cf-acc3-440e-a9e7-8ee06b4d5aa3",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "letter_df.plot(x=\"letter\", y=[\"all_tweets_relative_freq\", \"expected_relative_frequency\"], kind=\"barh\", figsize=(12,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b835be-513e-4e6a-8f21-32af5a8e61ef",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "# Chi-square test of independence.\n",
    "c, p, dof, expected = chi2_contingency(letter_df[['frequency', 'expected']])\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134ddd8b-f8b7-4cea-a42d-78306c54c0cd",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "letter_df[['frequency', 'expected']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb8082b-9c60-48cf-80c9-c9a0b65194db",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "df1 = df.copy()\n",
    "\n",
    "df1['number_of_characters'] = [len(tw) for tw in df1.tweet]\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd92d908-692f-428e-bcb0-238ef44c38dc",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "df1.number_of_characters.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd2d13a-8ae7-4070-9060-723ffb18fa40",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "df1.number_of_characters.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d5ffa1-8c00-4169-9d93-65d6d0b69547",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "df1.number_of_characters.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3774e7e-38ec-4685-95f1-382c04927f71",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "df1.number_of_characters.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dabc1a7-a5d0-40e5-97ce-e88600856f55",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "df1['number_of_words'] = [len(tw.split()) for tw in df1.tweet]\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c50ee8-0557-4275-9f6a-88d343948152",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "df1.number_of_words.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4da877-26ce-470a-9bb8-6d8560f6253c",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "df1.number_of_words.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b781add-77c6-4475-ad0a-6e7f03e1af51",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "df1.number_of_words.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489b1e72-c1a8-4d50-be53-a4bc9948657f",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "df1.number_of_words.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8143d555-789e-4d03-a870-db4c2e394284",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from wordcloud import WordCloud\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "all_tweets = ' '.join(df['tweet'].str.lower())\n",
    "\n",
    "f_words = [word for word in all_tweets.split()]\n",
    "counted_words = collections.Counter(f_words)\n",
    "\n",
    "words = []\n",
    "counts = []\n",
    "for letter, count in counted_words.most_common(20):\n",
    "    words.append(letter)\n",
    "    counts.append(count)\n",
    "\n",
    "plt.figure(figsize = (16, 4))\n",
    "plt.title('Most common words in whole tweets')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Words')\n",
    "plt.bar(words, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c74130-4e5e-4dd8-adda-1fc0392375c9",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "all_tweets = ' '.join(df[df.label == 'Positive'].tweet.str.lower())\n",
    "\n",
    "f_words = [word for word in all_tweets.split()]\n",
    "counted_words = collections.Counter(f_words)\n",
    "\n",
    "words = []\n",
    "counts = []\n",
    "for letter, count in counted_words.most_common(20):\n",
    "    words.append(letter)\n",
    "    counts.append(count)\n",
    "\n",
    "plt.figure(figsize = (16, 4))\n",
    "plt.title('Most common words in positive tweets')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Words')\n",
    "plt.bar(words, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd372138-4ecf-4dca-9f5e-9335b19e003d",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (25, 25))\n",
    "plt.axis('off')\n",
    "wordcloud_fig = WordCloud(max_words = 2000 , width = 1600 , height = 800, background_color ='white', min_font_size = 10).generate(\" \".join(df[df.label == 'Positive'].tweet))\n",
    "plt.imshow(wordcloud_fig, interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ffa179-9e48-40b5-9b9c-911381735446",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "all_tweets = ' '.join(df[df.label == 'Negative'].tweet.str.lower())\n",
    "\n",
    "f_words = [word for word in all_tweets.split()]\n",
    "counted_words = collections.Counter(f_words)\n",
    "\n",
    "words = []\n",
    "counts = []\n",
    "for letter, count in counted_words.most_common(20):\n",
    "    words.append(letter)\n",
    "    counts.append(count)\n",
    "\n",
    "plt.figure(figsize = (16, 4))\n",
    "plt.title('Most common words in negative tweets')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Words')\n",
    "plt.bar(words, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6870b2-bc12-4a80-ab05-56907f5c973c",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "plt.figure(figsize = (25, 25))\n",
    "plt.axis('off')\n",
    "wordcloud_fig = WordCloud(max_words = 2000 , width = 1600 , height = 800, background_color ='white', min_font_size = 10).generate(\" \".join(df[df.label == 'Negative'].tweet))\n",
    "plt.imshow(wordcloud_fig, interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ae9732-05ca-48cc-bee4-90d1f81aeeb0",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=7)\n",
    "print('Training Data', len(train_data), 'Test Data', len(test_data))\n",
    "\n",
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc10806-1137-4a34-ba51-fd91c9afff96",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "test_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321331dc-4593-4214-a093-a1b975ecfcf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T18:45:06.714933100Z",
     "start_time": "2024-05-06T18:45:03.595718900Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# Splitting the data into train and test sets (70% train, 30% test)\n",
    "train_data, test_data = train_test_split(train_data, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the text data in the training set\n",
    "X_train = tfidf_vectorizer.fit_transform(train_data['tweet'])  # Assuming 'tweet' is the text column\n",
    "\n",
    "# # Initialize the classifier and fit it to the TF-IDF transformed data\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "# clf = RandomForestClassifier()\n",
    "\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "# clf = SVC()\n",
    "\n",
    "\n",
    "clf.fit(X_train, train_data['label'])\n",
    "\n",
    "# Transform the test data using the same vectorizer\n",
    "X_test = tfidf_vectorizer.transform(test_data['tweet'])\n",
    "\n",
    "# Make predictions on the test data\n",
    "\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_data['label'], predictions)\n",
    "\n",
    "# Calculate recall, precision, and F1-score for the positive class\n",
    "recall_positive = recall_score(test_data['label'], predictions, pos_label='Positive')\n",
    "precision_positive = precision_score(test_data['label'], predictions, pos_label='Positive')\n",
    "f1_positive = f1_score(test_data['label'], predictions, pos_label='Positive')\n",
    "\n",
    "# Calculate recall, precision, and F1-score for the negative class\n",
    "recall_negative = recall_score(test_data['label'], predictions, pos_label='Negative')\n",
    "precision_negative = precision_score(test_data['label'], predictions, pos_label='Negative')\n",
    "f1_negative = f1_score(test_data['label'], predictions, pos_label='Negative')\n",
    "\n",
    "\n",
    "\n",
    "# Sensitivity is the same as recall in binary classification\n",
    "sensitivity = recall_positive\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Recall (Positive class):\", recall_positive)\n",
    "print(\"Precision (Positive class):\", precision_positive)\n",
    "print(\"F1-Score (Positive class):\", f1_positive)\n",
    "print(\"Recall (Negative class):\", recall_negative)\n",
    "print(\"Precision (Negative class):\", precision_negative)\n",
    "print(\"F1-Score (Negative class):\", f1_negative)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbfa3fed381f11a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(test_data['label'], predictions, labels=[\"Positive\", \"Negative\"])\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1cd200-8f6a-4130-b4ad-da1eb72d4942",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# Splitting the data into train and test sets (70% train, 30% test)\n",
    "train_data, test_data = train_test_split(train_data, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the text data in the training set\n",
    "X_train = tfidf_vectorizer.fit_transform(train_data['tweet'])  # Assuming 'tweet' is the text column\n",
    "\n",
    "# # Initialize the classifier and fit it to the TF-IDF transformed data\n",
    "\n",
    "# clf = DecisionTreeClassifier()\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "# clf = SVC()\n",
    "\n",
    "\n",
    "clf.fit(X_train, train_data['label'])\n",
    "\n",
    "# Transform the test data using the same vectorizer\n",
    "X_test = tfidf_vectorizer.transform(test_data['tweet'])\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_data['label'], predictions)\n",
    "\n",
    "# Calculate recall, precision, and F1-score for the positive class\n",
    "recall_positive = recall_score(test_data['label'], predictions, pos_label='Positive')\n",
    "precision_positive = precision_score(test_data['label'], predictions, pos_label='Positive')\n",
    "f1_positive = f1_score(test_data['label'], predictions, pos_label='Positive')\n",
    "\n",
    "# Calculate recall, precision, and F1-score for the negative class\n",
    "recall_negative = recall_score(test_data['label'], predictions, pos_label='Negative')\n",
    "precision_negative = precision_score(test_data['label'], predictions, pos_label='Negative')\n",
    "f1_negative = f1_score(test_data['label'], predictions, pos_label='Negative')\n",
    "\n",
    "\n",
    "\n",
    "# Sensitivity is the same as recall in binary classification\n",
    "sensitivity = recall_positive\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Recall (Positive class):\", recall_positive)\n",
    "print(\"Precision (Positive class):\", precision_positive)\n",
    "print(\"F1-Score (Positive class):\", f1_positive)\n",
    "print(\"Recall (Negative class):\", recall_negative)\n",
    "print(\"Precision (Negative class):\", precision_negative)\n",
    "print(\"F1-Score (Negative class):\", f1_negative)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6614964ca5e20409",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Initialize the Multinomial Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the TF-IDF transformed data\n",
    "clf.fit(X_train, train_data['label'])\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_data['label'], predictions)\n",
    "\n",
    "# Calculate recall, precision, and F1-score for the positive class\n",
    "recall_positive = recall_score(test_data['label'], predictions, pos_label='Positive')\n",
    "precision_positive = precision_score(test_data['label'], predictions, pos_label='Positive')\n",
    "f1_positive = f1_score(test_data['label'], predictions, pos_label='Positive')\n",
    "\n",
    "# Calculate recall, precision, and F1-score for the negative class\n",
    "recall_negative = recall_score(test_data['label'], predictions, pos_label='Negative')\n",
    "precision_negative = precision_score(test_data['label'], predictions, pos_label='Negative')\n",
    "f1_negative = f1_score(test_data['label'], predictions, pos_label='Negative')\n",
    "\n",
    "# Sensitivity is the same as recall in binary classification\n",
    "sensitivity = recall_positive\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Recall (Positive class):\", recall_positive)\n",
    "print(\"Precision (Positive class):\", precision_positive)\n",
    "print(\"F1-Score (Positive class):\", f1_positive)\n",
    "print(\"Recall (Negative class):\", recall_negative)\n",
    "print(\"Precision (Negative class):\", precision_negative)\n",
    "print(\"F1-Score (Negative class):\", f1_negative)\n",
    "print(\"Sensitivity:\", sensitivity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece171d8565912f4",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# Splitting the data into train and test sets (70% train, 30% test)\n",
    "train_data, test_data = train_test_split(train_data, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the text data in the training set\n",
    "X_train = tfidf_vectorizer.fit_transform(train_data['tweet'])  # Assuming 'tweet' is the text column\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "clf = SVC()\n",
    "\n",
    "\n",
    "clf.fit(X_train, train_data['label'])\n",
    "\n",
    "# Transform the test data using the same vectorizer\n",
    "X_test = tfidf_vectorizer.transform(test_data['tweet'])\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_data['label'], predictions)\n",
    "\n",
    "# Calculate recall, precision, and F1-score for the positive class\n",
    "recall_positive = recall_score(test_data['label'], predictions, pos_label='Positive')\n",
    "precision_positive = precision_score(test_data['label'], predictions, pos_label='Positive')\n",
    "f1_positive = f1_score(test_data['label'], predictions, pos_label='Positive')\n",
    "\n",
    "# Calculate recall, precision, and F1-score for the negative class\n",
    "recall_negative = recall_score(test_data['label'], predictions, pos_label='Negative')\n",
    "precision_negative = precision_score(test_data['label'], predictions, pos_label='Negative')\n",
    "f1_negative = f1_score(test_data['label'], predictions, pos_label='Negative')\n",
    "\n",
    "\n",
    "\n",
    "# Sensitivity is the same as recall in binary classification\n",
    "sensitivity = recall_positive\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Recall (Positive class):\", recall_positive)\n",
    "print(\"Precision (Positive class):\", precision_positive)\n",
    "print(\"F1-Score (Positive class):\", f1_positive)\n",
    "print(\"Recall (Negative class):\", recall_negative)\n",
    "print(\"Precision (Negative class):\", precision_negative)\n",
    "print(\"F1-Score (Negative class):\", f1_negative)\n",
    "print(\"Sensitivity:\", sensitivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bad73c-89d6-4bc1-b135-733c5e5ab013",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "print(\"Keras version:\", keras.__version__)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data.tweet)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Vocabulary Size :\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa41a61f-75c8-4ea8-bac5-50405e579dd2",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "MODELS_PATH = 'models'\n",
    "EMBEDDING_DIMENSION = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866d6d63-5473-4891-9c8d-ab9a5d66ae36",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 10\n",
    "LR = 1e-3\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "glove_file = open('glove/glove.6B.300d.txt', encoding='utf8')\n",
    "for line in glove_file:\n",
    "    values = line.split()\n",
    "    word = value = values[0]\n",
    "    coefficients = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefficients\n",
    "glove_file.close()\n",
    "\n",
    "print('%s word vectors.' % len(embeddings_index))\n",
    "\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIMENSION))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(vocab_size, EMBEDDING_DIMENSION, weights=[embedding_matrix], input_length=30, trainable=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
